"""
æ–‡ä»¶è·¯å¾„: services/api_engines/cnki.py
=========================================================
ã€å¯ç”¨æ¥å£è¯´æ˜ã€‘

class CnkiEngine(BaseEngine):
    def search(self, query: str) -> CitationData:
        '''
        ç­–ç•¥: ç™¾åº¦(é¦–é€‰) -> Bing(å¤‡é€‰) -> æœç‹—(ä¿åº•)
        è§£å†³: ç™¾åº¦403 & BingéªŒè¯ç é—®é¢˜
        '''
        pass
=========================================================
"""

# ==============================================================================
# ğŸ‘‡ 1. å¿…å¡«ï¼šè¯·å¡«å…¥æ‚¨çš„ç™¾åº¦ Cookie (è¿™æ˜¯æœ€ç¨³çš„æ–¹æ¡ˆï¼Œå¦‚æœä¸‹é¢ä»£ç è·‘ä¸é€šï¼Œè¯·åŠ¡å¿…å¡«è¿™ä¸ª)
MANUAL_COOKIE = ""
# ==============================================================================

import requests
from bs4 import BeautifulSoup
import time
import random
import logging
import re
import uuid
import os

from services.api_engines.base_engine import BaseEngine
from models.citation_model import CitationData
import config


class CnkiEngine(BaseEngine):
    """
    çŸ¥ç½‘ (CNKI) æœç´¢å¼•æ“ - ä¸‰é€šé“ç”Ÿå­˜ç‹‚ç‰ˆ (Baidu + Bing + Sogou)
    """

    def __init__(self):
        super().__init__()
        self.name = "CNKI_Proxy"
        self.session = requests.Session()

    def get_headers(self, source="baidu") -> dict:
        """æ ¹æ®ä¸åŒçš„æºç”Ÿæˆä¼ªè£…è¯·æ±‚å¤´"""
        # éšæœºé€‰ç”¨ä¸€ä¸ªæµè§ˆå™¨å¤´ï¼Œå¢åŠ é€šè¿‡ç‡
        ua_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        ua = random.choice(ua_list)

        headers = {
            'User-Agent': ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Connection': 'keep-alive'
        }

        if source == "baidu":
            headers['Referer'] = 'https://xueshu.baidu.com/'
            if MANUAL_COOKIE: headers['Cookie'] = MANUAL_COOKIE
        elif source == "bing":
            headers['Referer'] = 'https://www.bing.com/'
        elif source == "sogou":
            headers['Referer'] = 'https://scholar.sogou.com/'
            # æœç‹—æœ‰æ—¶å€™éœ€è¦ä¸€ä¸ªå‡çš„ Cookie æ‰èƒ½è·‘
            headers['Cookie'] = f'SUV={int(time.time() * 1000)};'

        return headers

    def search(self, query: str) -> CitationData:
        """æ€»å…¥å£ï¼šä¸‰çº§ç«ç®­ç­–ç•¥"""

        # 1. å°è¯•ç™¾åº¦ (æœ€å…¨)
        res = self.search_via_baidu(query)
        if res: return res

        # 2. å°è¯• Bing (æœ€å¿«)
        self.logger.warning(f"[{self.name}] ç™¾åº¦é€šé“å¤±æ•ˆï¼Œåˆ‡æ¢è‡³ Bing...")
        res = self.search_via_bing(query)
        if res: return res

        # 3. å°è¯• æœç‹— (æœ€åçš„å¸Œæœ›)
        self.logger.warning(f"[{self.name}] Bing é€šé“å¤±æ•ˆ (éªŒè¯ç )ï¼Œåˆ‡æ¢è‡³ æœç‹—(Sogou)...")
        return self.search_via_sogou(query)

    def search_via_baidu(self, query: str):
        url = "https://xueshu.baidu.com/s"
        params = {'wd': f"{query} site:cnki.net", 'tn': 'SE_baiduxueshu_c1gjeupa', 'ie': 'utf-8'}
        self.logger.info(f"[{self.name}] é€šé“ [1/3]: ç™¾åº¦å­¦æœ¯...")
        try:
            resp = self.session.get(url, params=params, headers=self.get_headers("baidu"), timeout=5)
            if resp.status_code == 200 and "éªŒè¯ç " not in resp.text:
                soup = BeautifulSoup(resp.text, 'html.parser')
                item = soup.find('div', class_='sc_content')
                if item: return self._parse_baidu_html(item)
            else:
                self.logger.warning(f"[{self.name}] ç™¾åº¦ 403/éªŒè¯ç ã€‚")
        except Exception:
            pass
        return None

    def search_via_bing(self, query: str):
        url = "https://www.bing.com/search"
        params = {'q': f"{query} site:cnki.net"}
        self.logger.info(f"[{self.name}] é€šé“ [2/3]: Bing...")
        try:
            resp = self.session.get(url, params=params, headers=self.get_headers("bing"), timeout=5)
            # Bing çš„éªŒè¯ç é¡µé¢ä¹Ÿæ˜¯ 200 OKï¼Œæ‰€ä»¥è¦æŸ¥å†…å®¹
            if "captcha" in resp.text or "challenge" in resp.url:
                self.logger.warning(f"[{self.name}] Bing è§¦å‘éªŒè¯ç ã€‚")
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            # å¯»æ‰¾ç»“æœåˆ—è¡¨
            items = soup.find_all('li', class_='b_algo')
            for item in items:
                data = self._parse_bing_html(item)
                if data and data.title: return data
        except Exception:
            pass
        return None

    def search_via_sogou(self, query: str):
        url = "https://scholar.sogou.com/xueshu"
        params = {'ie': 'utf-8', 'query': query}
        self.logger.info(f"[{self.name}] é€šé“ [3/3]: æœç‹—å­¦æœ¯...")

        try:
            resp = self.session.get(url, params=params, headers=self.get_headers("sogou"), timeout=8)
            resp.encoding = 'utf-8'

            if "éªŒè¯ç " in resp.text or "antispider" in resp.url:
                self.logger.warning(f"[{self.name}] æœç‹—ä¹Ÿè§¦å‘äº†éªŒè¯ç ...")
                # æœ€åçš„æŒ£æ‰ï¼šä¿å­˜æœç‹—é¡µé¢çœ‹çœ‹
                with open("debug_sogou_error.html", "w", encoding="utf-8") as f: f.write(resp.text)
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            # æœç‹—ç»“æœé€šå¸¸åœ¨ div.results > div.vrwrap
            results = soup.find_all('div', class_='vrwrap')

            if not results:
                self.logger.info(f"[{self.name}] æœç‹—æœªæ‰¾åˆ°ç»“æœã€‚")
                return None

            # æ‰¾ç¬¬ä¸€ä¸ªç»“æœ
            for item in results:
                # æœç‹—è§£æé€»è¾‘
                data = CitationData(entry_type="article", raw_data={"source": "Sogou"})

                # 1. æ ‡é¢˜ (h3.tit > a)
                h3 = item.find('h3', class_='tit')
                if h3 and h3.find('a'):
                    data.title = h3.find('a').get_text(strip=True)
                    data.url = "https://scholar.sogou.com" + h3.find('a').get('href', '')

                # 2. ä¿¡æ¯ (div.info)
                # æ ¼å¼: ä½œè€… - æœŸåˆŠ - å¹´ä»½
                info_div = item.find('div', class_='info')
                if info_div:
                    # æå–å¹´ä»½
                    text = info_div.get_text(" ", strip=True)
                    year_match = re.search(r'\b(19|20)\d{2}\b', text)
                    if year_match: data.year = year_match.group(0)

                    # å°è¯•æå–ä½œè€… (span.p1 æˆ–è€…æ˜¯ç¬¬ä¸€ä¸ª - ä¹‹å‰çš„å†…å®¹)
                    # æœç‹—æ¯”è¾ƒä¹±ï¼Œæˆ‘ä»¬ç®€å•åˆ†å‰²
                    parts = text.split('-')
                    if len(parts) >= 1:
                        # å‡è®¾ç¬¬ä¸€éƒ¨åˆ†æ˜¯ä½œè€…
                        data.authors = parts[0].strip().split(',')
                    if len(parts) >= 2:
                        # å‡è®¾ç¬¬äºŒéƒ¨åˆ†æ˜¯æœŸåˆŠ
                        possible_journal = parts[1].strip()
                        if not possible_journal.isdigit():
                            data.source = possible_journal

                if data.title:
                    return data

        except Exception as e:
            self.logger.error(f"[{self.name}] æœç‹—é€šé“å‡ºé”™: {e}")

        return None

    def _parse_baidu_html(self, item_soup) -> CitationData:
        """å¤ç”¨ä¹‹å‰çš„ç™¾åº¦è§£æ"""
        citation = CitationData(entry_type="article", raw_data={"source": "Baidu"})
        try:
            t = item_soup.find('h3', class_='t')
            if t and t.find('a'): citation.title = t.find('a').get_text(strip=True)

            info = item_soup.find('div', class_='sc_info')
            if info:
                txt = info.get_text(" ", strip=True)
                ym = re.search(r'\b(19|20)\d{2}\b', txt)
                if ym: citation.year = ym.group(0)

                js = info.find('span', class_='sc_journal')
                if js: citation.source = js.get_text(strip=True)
        except:
            pass
        return citation

    def _parse_bing_html(self, item_soup) -> CitationData:
        """å¤ç”¨ä¹‹å‰çš„ Bing è§£æ"""
        citation = CitationData(entry_type="article", raw_data={"source": "Bing"})
        try:
            h2 = item_soup.find('h2')
            if h2 and h2.find('a'):
                citation.title = h2.find('a').get_text(strip=True)
                citation.url = h2.find('a').get('href', '')

            cap = item_soup.find('div', class_='b_caption')
            if cap:
                txt = cap.get_text(" ", strip=True)
                ym = re.search(r'\b(19|20)\d{2}\b', txt)
                if ym: citation.year = ym.group(0)
        except:
            pass
        return citation